{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lovehiku/-Mini_Expense-Splitter/blob/main/Copy_of_ML_LAB_01_Naive_Bayes_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np #python array only containing number\n",
        "import matplotlib.pyplot as plt #used to plot\n",
        "from sklearn.datasets import load_iris #optmaized dataset lmtekem\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from collections import defaultdict\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "metadata": {
        "id": "v_v-TCMlK2qh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load and Prepare Data"
      ],
      "metadata": {
        "id": "GpAPvAixK-TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 1. Data Loading and Preparation ---\") //\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names #sepal length width so on\n",
        "target_names = iris.target_names #setosa  so on"
      ],
      "metadata": {
        "id": "qlWLdRfuK8Qw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "3945f336-ea50-4118-8179-270d38374541"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1415581771.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1415581771.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(\"--- 1. Data Loading and Preparation ---\") //\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names"
      ],
      "metadata": {
        "id": "Phli4swoUre_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names # species"
      ],
      "metadata": {
        "id": "5Spy14Z1VGNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "\n"
      ],
      "metadata": {
        "id": "nLQT0go2LA7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)#shuffle same way that what random state work\n",
        "# training ,validtaion & test\n",
        "print(f\"Total samples: {len(X)}\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "print(f\"Features (n): {feature_names}\")\n",
        "print(f\"Classes (K): {target_names}\")\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "IRWygwkpLHr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization"
      ],
      "metadata": {
        "id": "j_1l6xb1LJdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data_distribution(X, y, feature_names, target_names, x_axis_idx, y_axis_idx):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for i, target_name in enumerate(target_names):\n",
        "\n",
        "        plt.scatter(\n",
        "            X[y == i, x_axis_idx],\n",
        "            X[y == i, y_axis_idx],\n",
        "            label=target_name,\n",
        "            alpha=0.7\n",
        "        )\n",
        "\n",
        "    plt.title(\"Iris Dataset: Feature Distribution by Class\")\n",
        "    plt.xlabel(feature_names[x_axis_idx])\n",
        "    plt.ylabel(feature_names[y_axis_idx])\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6xF3-02DLOT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 1.5 Data Visualization: Features 1 vs 2 ---\")\n",
        "plot_data_distribution(X, y, feature_names, target_names, 1, 2)\n",
        "print(\"Interpretation: Naive Bayes models the mean and variance (spread) of these clusters for each class.\\n\")"
      ],
      "metadata": {
        "id": "BqAmwpfyLZS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets build our classifier"
      ],
      "metadata": {
        "id": "MVWEL6qPMcP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayesClassifier:\n",
        "    def fit(self, X, y):\n",
        "        # Determine unique classes (K) and number of features (n)\n",
        "        self.classes = np.unique(y) # find unique target classes\n",
        "        self.n_features = X.shape[1] # find number of features\n",
        "\n",
        "        # Initialize dictionaries to store mean, variance, and prior probabilities\n",
        "        self.class_mean = {}\n",
        "        self.class_var = {}\n",
        "        self.priors = {}\n",
        "\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c] # all data to specific class\n",
        "\n",
        "            # 2a. Calculate Prior P(C_k)\n",
        "            # Prior = (Number of samples in class c) / (Total samples)\n",
        "            self.priors[c] = len(X_c) / len(X)\n",
        "\n",
        "            # 2b. Calculate Mean and Variance for each feature (for Likelihood)\n",
        "            self.class_mean[c] = X_c.mean(axis=0)\n",
        "            self.class_var[c] = X_c.var(axis=0)\n",
        "\n",
        "    # Helper function: Gaussian Probability Density Function (PDF)\n",
        "    def _gaussian_pdf(self, x, mean, var):\n",
        "        # P(x_i | C_k) calculation based on Gaussian distribution\n",
        "        epsilon = 1e-6  # Small value to prevent division by zero in variance\n",
        "        numerator = np.exp(-((x - mean) ** 2) / (2 * (var + epsilon)))\n",
        "        denominator = np.sqrt(2 * np.pi * (var + epsilon))\n",
        "        return numerator / denominator\n",
        "\n",
        "    def predict(self, X):e\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # Dictionary to hold the final score for each class\n",
        "        posterior_scores = {}\n",
        "\n",
        "        for c in self.classes:\n",
        "            prior = self.priors[c]\n",
        "            likelihood = 1.0\n",
        "\n",
        "            # Naive Likelihood Calculation (Product Rule)\n",
        "            for i in range(self.n_features):\n",
        "                # Calculate P(x_i | C_k)\n",
        "                feature_likelihood = self._gaussian_pdf(\n",
        "                    x[i],\n",
        "                    self.class_mean[c][i],\n",
        "                    self.class_var[c][i]\n",
        "                )\n",
        "                # Apply Naive Independence Assumption: P(X|y) = P(x1|y) * P(x2|y) * ...\n",
        "                likelihood *= feature_likelihood\n",
        "\n",
        "            # Posterior Score = Likelihood * Prior\n",
        "            posterior_score = likelihood * prior\n",
        "            posterior_scores[c] = posterior_score\n",
        "\n",
        "        # Argmax Decision: Pick the class (k) that yields the highest score.\n",
        "        predicted_class = max(posterior_scores, key=posterior_scores.get)\n",
        "\n",
        "        return predicted_class\n"
      ],
      "metadata": {
        "id": "ahGvVZtBMeWv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "616a79ed-f277-427a-dd65-b36e6571cf83"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (ipython-input-4291173844.py, line 32)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4291173844.py\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    predictions = [self._predict(x) for x in X]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training and Demonstration**"
      ],
      "metadata": {
        "id": "u5VDsib_MjI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb = NaiveBayesClassifier()\n",
        "nb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "cY4x5oSiMnGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Results (Prior and Feature Means/Variances)"
      ],
      "metadata": {
        "id": "qoOQMddYM8r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 2. Training Results (Prior and Feature Means/Variances) ---\")\n",
        "for c in nb.classes:\n",
        "    class_name = target_names[c]\n",
        "    print(f\"Class: {class_name} ({c})\")\n",
        "    print(f\"  Prior P({class_name}): {nb.priors[c]:.4f}\")\n",
        "    # Show mean and variance for all features\n",
        "    for i in range(nb.n_features):\n",
        "        print(f\"    - '{feature_names[i]}': Mean={nb.class_mean[c][i]:.4f}, Var={nb.class_var[c][i]:.4f}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "glM2sGHIM6fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample_index = 0\n",
        "x_test_single = X_test[test_sample_index]\n",
        "true_class_label = target_names[y_test[test_sample_index]]\n",
        "\n",
        "print(\"\\n--- 3. Detailed Trace for a Single Sample (The Argmax Input) ---\")\n",
        "print(f\"Test Features (X): {x_test_single}\")\n",
        "print(f\"True Class: {true_class_label}\\n\")"
      ],
      "metadata": {
        "id": "XG7ugBp3N_Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 6. Normal Distribution Visualization for Each Feature and Class (with Test Sample Highlight) ---\")\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Define a range for x-axis for plotting PDFs, based on min/max of each feature\n",
        "feature_ranges = []\n",
        "for i in range(nb.n_features):\n",
        "    min_val = np.min(X[:, i])\n",
        "    max_val = np.max(X[:, i])\n",
        "    feature_ranges.append(np.linspace(min_val - 0.5, max_val + 0.5, 1000)) # this will create evenly spaced thing for the chart we are going to build\n"
      ],
      "metadata": {
        "id": "NmVSYlBKbGBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get default color cycle for consistency\n",
        "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
        "colors = prop_cycle.by_key()['color']\n"
      ],
      "metadata": {
        "id": "FCvvimQibh2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb.class_mean"
      ],
      "metadata": {
        "id": "WrIhuE0gb9co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_single"
      ],
      "metadata": {
        "id": "1SxOqgvvc9Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i in range(nb.n_features):\n",
        "    plt.subplot(2, 2, i + 1) # Arrange plots in a 2x2 grid rows, columns then index\n",
        "    x_values = feature_ranges[i]\n",
        "\n",
        "    # Store PDF values for test sample for each class to plot markers later\n",
        "    test_sample_pdf_values = {}\n",
        "\n",
        "    for c_idx, c in enumerate(nb.classes):\n",
        "        class_name = target_names[c]\n",
        "        mean = nb.class_mean[c][i] # mean of some class and some feature/column\n",
        "        var = nb.class_var[c][i] # var of some class and some feature/column\n",
        "\n",
        "        # Calculate PDF values for the curve\n",
        "        pdf_values = [nb._gaussian_pdf(x_val, mean, var) for x_val in x_values]\n",
        "\n",
        "        plt.plot(x_values, pdf_values, label=f'{class_name}', color=colors[c_idx])\n",
        "\n",
        "        # Calculate PDF value for the specific test sample point\n",
        "        test_sample_pdf_value = nb._gaussian_pdf(x_test_single[i], mean, var)\n",
        "        test_sample_pdf_values[c] = test_sample_pdf_value\n",
        "\n",
        "        # Plot a marker for the test sample on this class's curve\n",
        "        plt.scatter(x_test_single[i], test_sample_pdf_value, color=colors[c_idx], s=100, marker='X', zorder=5)\n",
        "\n",
        "\n",
        "    # Add a single vertical line for the selected test sample's feature value\n",
        "    # Only add to legend once\n",
        "    if i == 0:\n",
        "      plt.axvline(x=x_test_single[i], color='red', linestyle='--', label='Test Sample Value')\n",
        "    else:\n",
        "      plt.axvline(x=x_test_single[i], color='red', linestyle='--')\n",
        "\n",
        "    plt.title(f'Feature: {feature_names[i]}')\n",
        "    plt.xlabel('Value')\n",
        "    plt.ylabel('Probability Density')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Interpretation: Each plot shows the Gaussian distribution for one feature, separated by class. The Naive Bayes classifier uses these distributions to determine the likelihood of a new data point belonging to each class.\")\n",
        "print(\"The red dashed line indicates the value of the selected test sample for that specific feature. The 'X' markers on each class's curve show the probability density (likelihood) of that sample's feature value for each respective class.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "4pb3V8RRNH6_",
        "outputId": "84398a02-a057-4f0a-d459-703348ae0254"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nb' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-316701967.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Arrange plots in a 2x2 grid rows, columns then index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_ranges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nb' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manually trace the prediction process for the single sample"
      ],
      "metadata": {
        "id": "0f1oQo_6NkG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class_index = nb.predict(np.array([x_test_single]))[0]\n",
        "predicted_class_label = target_names[predicted_class_index]\n",
        "\n",
        "print(\"\\n--- 3. Prediction for a Single Sample using nb.predict() ---\")\n",
        "print(f\"Test Features (X): {x_test_single}\")\n",
        "print(f\"True Class: {true_class_label}\\n\")\n",
        "print(f\"Predicted Class (using nb.predict()): {predicted_class_label} (Index: {predicted_class_index})\")\n"
      ],
      "metadata": {
        "id": "eQcxIO1gOhxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fa2a61b"
      },
      "source": [
        "# --- 5. Full Model Evaluation ---\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print(\"\\n--- 5. Model Evaluation Metrics ---\")\n",
        "\n",
        "# 5a. Classification Report\n",
        "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
        "print(f\"Overall Accuracy on Test Set: {accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "print(\"--- Classification Report (Precision, Recall, F1-Score) ---\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# 5b. Confusion Matrix Visualization\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
        "\n",
        "print(\"--- Confusion Matrix ---\")\n",
        "# Plotting the confusion matrix for visual interpretation\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "disp.plot(ax=ax, cmap=plt.cm.Blues)\n",
        "ax.set_title(\"Confusion Matrix for Naive Bayes Classifier\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation: The confusion matrix shows how many samples were correctly (diagonal) and incorrectly (off-diagonal) classified.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Is there a simple way to do those predictions, instead of doing all those math?"
      ],
      "metadata": {
        "id": "qEwV7p6vfRez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Scikit-learn GaussianNB Predictions on Test Set ---\")\n",
        "\n",
        "# Instantiate and train scikit-learn's Gaussian Naive Bayes\n",
        "sk_gnb = GaussianNB()\n",
        "sk_gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "sk_y_pred = sk_gnb.predict(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "1VeuWrXufWNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cc620f5"
      },
      "source": [
        "print(\"\\n--- Scikit-learn GaussianNB Model Evaluation Metrics ---\")\n",
        "\n",
        "# Calculate and print overall accuracy\n",
        "sk_accuracy = np.sum(sk_y_pred == y_test) / len(y_test)\n",
        "print(f\"Overall Accuracy on Test Set (Scikit-learn GNB): {sk_accuracy * 100:.2f}%\\n\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"--- Scikit-learn GNB Classification Report (Precision, Recall, F1-Score) ---\")\n",
        "print(classification_report(y_test, sk_y_pred, target_names=target_names))\n",
        "\n",
        "# Generate and display confusion matrix\n",
        "sk_cm = confusion_matrix(y_test, sk_y_pred)\n",
        "sk_disp = ConfusionMatrixDisplay(confusion_matrix=sk_cm, display_labels=target_names)\n",
        "\n",
        "print(\"\\n--- Scikit-learn GNB Confusion Matrix ---\")\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "sk_disp.plot(ax=ax, cmap=plt.cm.Greens)\n",
        "ax.set_title(\"Confusion Matrix for Scikit-learn GaussianNB Classifier\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterpretation: This shows the performance of the scikit-learn Gaussian Naive Bayes classifier on the test set, including its accuracy, precision, recall, f1-score, and a visual confusion matrix.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}